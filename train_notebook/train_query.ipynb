{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79f721a-ca32-468c-a93b-24c8f9285071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/ml/develop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4676d6a-dc4f-4e6f-9a8b-9bc5646de4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from args_jupyter import parse_args\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "from dkt.utils import setSeeds\n",
    "from dkt.dataloader import get_loaders\n",
    "from dkt.optimizer import get_optimizer, get_lr\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.criterion import get_criterion\n",
    "from dkt.metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286c33de-3825-418d-8a27-252e81914bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(mode='train')\n",
    "setSeeds(args.seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0cf579-64e8-4947-98a5-40cdf6894107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "\n",
    "    test, question, tag, correct, mask = batch\n",
    "    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #    saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, gather_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a04ef2d-a666-4c95-a95d-f9043a368af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(args.file_name)\n",
    "train_data = preprocess.get_train_data()\n",
    "\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25bf1672-12da-4592-8a6d-d8bc24c02856",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2140465-5ef1-4657-a0c2-5bfe039ef9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testID size: torch.Size([64, 20])\n",
      "assessmentItemId size: torch.Size([64, 20])\n",
      "knowledgetag size: torch.Size([64, 20])\n",
      "answercode size: torch.Size([64, 20])\n",
      "mask size: torch.Size([64, 20])\n"
     ]
    }
   ],
   "source": [
    "batch = None\n",
    "for t in train_loader:\n",
    "    # train_loader is tuple\n",
    "    \n",
    "    print(f\"testID size: {t[0].size()}\")\n",
    "    print(f\"assessmentItemId size: {t[1].size()}\")\n",
    "    print(f\"knowledgetag size: {t[2].size()}\")\n",
    "    print(f\"answercode size: {t[3].size()}\")\n",
    "    print(f\"mask size: {t[4].size()}\")\n",
    "    \n",
    "    batch = t\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a87d848-cb61-4a16-890d-35a3ce19dcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(asset_dir='asset/', batch_size=64, clip_grad=10, data_dir='/opt/ml/input/data/train_dataset', data_id='userID', device='cpu', drop_out=0.2, emb_size=100, file_name='train_data.csv', hidden_dim=64, log_steps=50, lr=0.0001, max_lr=0.0001, max_seq_len=20, min_lr=1e-05, model='lstm', model_dir='models/', model_name='model.pt', n_epochs=20, n_heads=2, n_layers=2, n_questions=9455, n_tag=913, n_test=1538, num_workers=1, optimizer='adam', output_dir='output/', patience=5, scheduler='plateau', scheduler_step=5, seed=5, test_file_name='test_data.csv', wandb_name=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda08c9c-e1e9-4aee-aff0-4ccd0c56dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3e1585-22f2-4b9a-87ec-ad26905fa93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = process_batch(batch, args)\n",
    "# test, question, tag, correct, mask, interaction, gather_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c3bcbc-837b-4a83-ac92-18c95e79cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test, question, tag, _, mask, interaction, index = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e718b0-ed26-4da7-996d-6bf952166c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = interaction.size(0)\n",
    "seq_len = interaction.size(1)\n",
    "batch_size, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d214bd30-2ab1-432b-be18-5dcc6756f82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9b0a30-a7de-4861-8d1d-7e4587593b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "313a1671-4823-45f0-8610-0e5ea99cccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1, temp2 = mask.view(args.batch_size, args.max_seq_len, -1).max(2)\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ce945-0d56-4eb1-be55-96acc4c7506f",
   "metadata": {},
   "source": [
    "# Last Query 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64817a0-0f45-4165-83c5-28dfa107b2fd",
   "metadata": {},
   "source": [
    "### Post Padding 사용시\n",
    "- post padding을 사용할 경우 last query 구현이 조금 까다롭다. Tensor의 변화 흐름을 한번 쯤 느껴보며 아래 2가지 살펴보기\n",
    "- 3D tensor에서 원하는 last query 데이터만 가져오기 위한 `gather`와 `index` 사용법\n",
    "- last query를 위한 3D mask 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb801bc3-85a9-4581-ad07-5ace65d2ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "\n",
    "    def forward(self,ffn_in):\n",
    "        return self.layer2(F.relu(self.layer1(ffn_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b719a8fa-05a5-4464-9622-4cb5363037f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = interaction.size(0)\n",
    "seq_len = interaction.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0360bed9-98a9-41f0-b143-5728dc0f472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 21])\n",
      "torch.Size([64, 20, 21])\n",
      "torch.Size([64, 20, 21])\n",
      "torch.Size([64, 20, 21])\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "# interacton은 현재 correct으로 구성되어있다\n",
    "# correct(1, 2) + padding(0)\n",
    "embedding_interaction = nn.Embedding(3, args.hidden_dim//3)\n",
    "embedding_test = nn.Embedding(args.n_test + 1, args.hidden_dim//3)\n",
    "embedding_question = nn.Embedding(args.n_questions + 1, args.hidden_dim//3)\n",
    "embedding_tag = nn.Embedding(args.n_tag + 1, args.hidden_dim//3)\n",
    "embedding_position = nn.Embedding(args.max_seq_len, args.hidden_dim)\n",
    "\n",
    "# 신나는 embedding\n",
    "embed_interaction = embedding_interaction(interaction)\n",
    "embed_test = embedding_test(test)\n",
    "embed_question = embedding_question(question)\n",
    "embed_tag = embedding_tag(tag)\n",
    "\n",
    "print(embed_interaction.size())\n",
    "print(embed_test.size())\n",
    "print(embed_question.size())\n",
    "print(embed_tag.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8895f3-9f75-46bf-948e-794da2561d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 84])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.cat([embed_interaction,\n",
    "                   embed_test,\n",
    "                   embed_question,\n",
    "                   embed_tag,], 2)\n",
    "embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6571e974-be86-44f9-95f8-8980cb42c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_proj = nn.Linear((args.hidden_dim//3)*4, args.hidden_dim)\n",
    "embed = comb_proj(embed)\n",
    "embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c410b6-2c08-4f62-820a-c5c2103b2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 keetar님 솔루션에서는 Positional Embedding 사용되지 않는다\n",
    "# 사용 여부 자유롭게 결정해주세요 :)\n",
    "\n",
    "# self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0b7d7ef-77f1-42d4-a4c8-3924c3cef9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "query = nn.Linear(in_features=args.hidden_dim, out_features=args.hidden_dim)\n",
    "key = nn.Linear(in_features=args.hidden_dim, out_features=args.hidden_dim)\n",
    "value = nn.Linear(in_features=args.hidden_dim, out_features=args.hidden_dim)\n",
    "\n",
    "q = query(embed)\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "108a7ed4-54cd-4a91-8928-994827a357ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 3D gathering은 머리가 아픕니다. 잠시 머리를 식히고 옵니다.\n",
    "q = torch.gather(q, 1, index.repeat(1, args.hidden_dim).unsqueeze(1))\n",
    "q = q.permute(1, 0, 2)\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb608ce-035f-4b29-86a2-8a2bd0ac6b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 64, 64]), torch.Size([20, 64, 64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = key(embed).permute(1, 0, 2)\n",
    "v = value(embed).permute(1, 0, 2)\n",
    "k.size(), v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b913d745-7ee7-442b-be0b-494fb787540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(seq_len, index, batch_size):\n",
    "    \"\"\"\n",
    "    batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n",
    "\n",
    "    참고로 (batch_size*self.args.n_heads, seq_len, seq_len) 가 아니라\n",
    "          (batch_size*self.args.n_heads,       1, seq_len) 로 하는 이유는\n",
    "\n",
    "    last query라 output의 seq부분의 사이즈가 1이기 때문이다\n",
    "    \"\"\"\n",
    "    # [[1], -> [1, 2, 3]\n",
    "    #  [2],\n",
    "    #  [3]]\n",
    "    index = index.view(-1)\n",
    "\n",
    "    # last query의 index에 해당하는 upper triangular mask의 row를 사용한다\n",
    "    mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1))\n",
    "    mask = mask[index]\n",
    "\n",
    "    # batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n",
    "    mask = mask.repeat(1, args.n_heads).view(batch_size*args.n_heads, -1, seq_len)\n",
    "    return mask.masked_fill(mask==1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b408593e-c152-4025-8b72-820c6db67c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.MultiheadAttention(embed_dim=args.hidden_dim, num_heads=args.n_heads)\n",
    "mask = None # last query에서는 필요가 없지만 수정을 고려하여서 넣어둠\n",
    "ffn = Feed_Forward_block(args.hidden_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c68445c1-a84b-4f32-aa1e-428c2c304eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 20])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention\n",
    "# last query only\n",
    "mask = get_mask(seq_len, index, batch_size)\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fccfd282-5864-4604-87c5-e3fe827f7451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = attn(q, k, v, attn_mask=mask)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd7de6e4-3e76-4424-850d-da7bb7ae0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 64])\n",
      "torch.Size([64, 20, 64])\n",
      "torch.Size([64, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "ln1 = nn.LayerNorm(args.hidden_dim)\n",
    "ln2 = nn.LayerNorm(args.hidden_dim)\n",
    "\n",
    "## residual + layer norm\n",
    "out = out.permute(1, 0, 2)\n",
    "out = embed + out\n",
    "out = ln1(out)\n",
    "print(out.size())\n",
    "\n",
    "## feed forward network\n",
    "out = ffn(out)\n",
    "print(out.size())\n",
    "\n",
    "## residual + layer norm\n",
    "out = embed + out\n",
    "out = ln2(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8805165-9f2a-4d89-bc15-75da86d48c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(batch_size):\n",
    "    h = torch.zeros(\n",
    "        args.n_layers,\n",
    "        batch_size,\n",
    "        args.hidden_dim)\n",
    "#     h = h.to(self.device)\n",
    "\n",
    "    c = torch.zeros(\n",
    "        args.n_layers,\n",
    "        batch_size,\n",
    "        args.hidden_dim)\n",
    "#     c = c.to(self.device)\n",
    "\n",
    "    return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d5988d-49bd-461a-bf2f-028c6583464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n",
      "torch.Size([2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "lstm = nn.LSTM(\n",
    "    args.hidden_dim,\n",
    "    args.hidden_dim,\n",
    "    args.n_layers,\n",
    "    batch_first=True)\n",
    "\n",
    "hidden = init_hidden(batch_size)\n",
    "out, hidden = lstm(out, hidden)\n",
    "print(out[0].shape)\n",
    "print(hidden[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7718bcfb-461d-420b-aec0-4d82f9281f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 64])\n",
      "torch.Size([64, 20, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully connected layer\n",
    "fc = nn.Linear(args.hidden_dim, 1)\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "out = out.contiguous().view(batch_size, -1, args.hidden_dim)\n",
    "print(out.size())\n",
    "out = fc(out)\n",
    "print(out.size())\n",
    "\n",
    "preds = activation(out).view(batch_size, -1)\n",
    "preds.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8178f7-7060-4cd9-8d8d-f180f740157e",
   "metadata": {},
   "source": [
    "### Class로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aadd6d2-fdff-451d-9fc1-e146c331ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "\n",
    "    def forward(self,ffn_in):\n",
    "        return self.layer2(F.relu(self.layer1(ffn_in)))\n",
    "\n",
    "class LastQuery(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(LastQuery, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        \n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        # 기존 keetar님 솔루션에서는 Positional Embedding은 사용되지 않습니다\n",
    "        # 하지만 사용 여부는 자유롭게 결정해주세요 :)\n",
    "        # self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.query = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.key = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.value = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=self.args.n_heads)\n",
    "        self.mask = None # last query에서는 필요가 없지만 수정을 고려하여서 넣어둠\n",
    "        self.ffn = Feed_Forward_block(self.hidden_dim)      \n",
    "\n",
    "        self.ln1 = nn.LayerNorm(self.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.hidden_dim,\n",
    "            self.hidden_dim,\n",
    "            self.args.n_layers,\n",
    "            batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def get_mask(self, seq_len, index, batch_size):\n",
    "        \"\"\"\n",
    "        batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n",
    "        \n",
    "        참고로 (batch_size*self.args.n_heads, seq_len, seq_len) 가 아니라\n",
    "              (batch_size*self.args.n_heads,       1, seq_len) 로 하는 이유는\n",
    "        \n",
    "        last query라 output의 seq부분의 사이즈가 1이기 때문이다\n",
    "        \"\"\"\n",
    "        # [[1], -> [1, 2, 3]\n",
    "        #  [2],\n",
    "        #  [3]]\n",
    "        index = index.view(-1)\n",
    "\n",
    "        # last query의 index에 해당하는 upper triangular mask의 row를 사용한다\n",
    "        mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1))\n",
    "        mask = mask[index]\n",
    "\n",
    "        # batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n",
    "        mask = mask.repeat(1, self.args.n_heads).view(batch_size*self.args.n_heads, -1, seq_len)\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def get_pos(self, seq_len):\n",
    "        # use sine positional embeddinds\n",
    "        return torch.arange(seq_len).unsqueeze(0)\n",
    " \n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.args.n_layers,\n",
    "            batch_size,\n",
    "            self.args.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.args.n_layers,\n",
    "            batch_size,\n",
    "            self.args.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, index = input\n",
    "        batch_size = interaction.size(0)\n",
    "        seq_len = interaction.size(1)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        embed = self.comb_proj(embed)\n",
    "\n",
    "        # Positional Embedding\n",
    "        # last query에서는 positional embedding을 하지 않음\n",
    "        # position = self.get_pos(seq_len).to('cuda')\n",
    "        # embed_pos = self.embedding_position(position)\n",
    "        # embed = embed + embed_pos\n",
    "\n",
    "        ####################### ENCODER #####################\n",
    "        q = self.query(embed)\n",
    "\n",
    "        # 이 3D gathering은 머리가 아픕니다. 잠시 머리를 식히고 옵니다.\n",
    "        q = torch.gather(q, 1, index.repeat(1, self.hidden_dim).unsqueeze(1))\n",
    "        q = q.permute(1, 0, 2)\n",
    "\n",
    "        k = self.key(embed).permute(1, 0, 2)\n",
    "        v = self.value(embed).permute(1, 0, 2)\n",
    "\n",
    "        ## attention\n",
    "        # last query only\n",
    "        self.mask = self.get_mask(seq_len, index, batch_size).to(self.device)\n",
    "        out, _ = self.attn(q, k, v, attn_mask=self.mask)\n",
    "        \n",
    "        ## residual + layer norm\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = embed + out\n",
    "        out = self.ln1(out)\n",
    "\n",
    "        ## feed forward network\n",
    "        out = self.ffn(out)\n",
    "\n",
    "        ## residual + layer norm\n",
    "        out = embed + out\n",
    "        out = self.ln2(out)\n",
    "\n",
    "        ###################### LSTM #####################\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(out, hidden)\n",
    "\n",
    "        ###################### DNN #####################\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc8da37b-5326-44d7-a05d-fb8c631c01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size : torch.Size([64, 20])\n"
     ]
    }
   ],
   "source": [
    "model = LastQuery(args)\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "    inputs = process_batch(batch, args)\n",
    "    output = model(inputs)\n",
    "    print(f\"output size : {output.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2562392e-feef-4fac-a570-3cf7495d4703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4927, 0.4948, 0.4984,  ..., 0.5102, 0.5111, 0.5109],\n",
       "        [0.4928, 0.4939, 0.4935,  ..., 0.4834, 0.4860, 0.4862],\n",
       "        [0.4887, 0.4880, 0.4867,  ..., 0.4905, 0.4858, 0.4844],\n",
       "        ...,\n",
       "        [0.4900, 0.4838, 0.4813,  ..., 0.4942, 0.4984, 0.5012],\n",
       "        [0.4880, 0.4864, 0.4850,  ..., 0.4905, 0.4940, 0.5000],\n",
       "        [0.4897, 0.4874, 0.4914,  ..., 0.4937, 0.4915, 0.4902]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88189fd3-dc00-49e3-87c5-2d5f6159412a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
